{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6ZCGIqxlec2+aAlU9IacE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woojung02/SSAC_AI/blob/main/Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # PyTorch 라이브러리 임포트 (딥러닝 프레임워크)\n",
        "import torch.nn as nn  # 신경망 모듈\n",
        "import torch.optim as optim  # 최적화 알고리즘\n",
        "import torchvision  # 이미지 데이터셋과 전처리 도구\n",
        "import torchvision.transforms as transforms  # 이미지 변환 함수들\n",
        "\n",
        "# GPU 사용 가능하면 'cuda', 아니면 'cpu'를 사용하기 위한 장치(device) 지정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# CIFAR-10 학습 데이터셋에 적용할 이미지 전처리 및 데이터 증강 정의\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # 원본 이미지 주변에 4픽셀 패딩 후 32x32 영역 랜덤 크롭 (과적합 방지)\n",
        "    transforms.RandomHorizontalFlip(),  # 좌우 반전 (데이터 다양성 증대)\n",
        "    transforms.ToTensor(),  # 이미지를 텐서로 변환 (0~1 값)\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),  # RGB 3채널 평균값으로 정규화(직접 계산해서 넣어준다. 이유:입력데이터가 일정한 값을 가지면 안정도,속도가 올라간다.)\n",
        "                         (0.2023, 0.1994, 0.2010))  # RGB 3채널 표준편차로 정규화(평균값으로 정규화한 이유와 같은 이유)\n",
        "])\n",
        "\n",
        "# 테스트 데이터셋 전처리\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# CIFAR-10 학습 데이터셋 다운로드 및 전처리 적용(CIFAR-10은 50000(학습용)+10000(테스트용)으로 구성 되어있고,32*32의 RGB로 구성된다.10개의 클라스로 분류되고 (배,차,고양이,강아지)등 크기가 작은 이미지로 이루어져 있다.)\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "# 학습 데이터셋을 배치 크기 128, 셔플 옵션 켜서 로드(shuffle이란 배치 데이터를 무작위로 섞는거,사용 이유:데이터가 순서대로 학습하면 편향될수 있고 ,과적합을 줄일수 있다. 즉 순서에 의존하는것을 막을수 있다.)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# CIFAR-10 테스트 데이터셋 다운로드 및 전처리 적용\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "# 테스트 데이터셋 배치 크기 100, 셔플 없음(셔플=false인 이유:학습 단계가 아닌 테스트 단계이므로 일관된 평가를 해야하니 사용 하지 않음)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "#  ResidualBlock 정의(잔차 학습은 입력함수x와 변환함수 H(x)의 차이를 학습한다.이유:기울기가 0에 가까워지는 문제(기울기 소실)을 해결할수 있다)\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        # 첫번째 3x3 합성곱, stride와 padding=1로 공간 크기 조절, bias는 batchnorm 때문에 False\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)  # 배치 정규화\n",
        "        self.relu = nn.ReLU(inplace=True)  # ReLU 활성화 함수, 메모리 절약 inplace=True\n",
        "        # 두번째 3x3 합성곱, stride=1로 공간 크기 유지\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample  # 입력과 출력 차원 다르면 맞춰주는 레이어\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # 입력을 저장 (skip connection 용)\n",
        "\n",
        "        out = self.conv1(x)  # 첫 합성곱 수행\n",
        "        out = self.bn1(out)  # 배치 정규화\n",
        "        out = self.relu(out)  # ReLU 활성화\n",
        "\n",
        "        out = self.conv2(out)  # 두번째 합성곱 수행\n",
        "        out = self.bn2(out)  # 배치 정규화\n",
        "\n",
        "        # 입력과 출력 크기가 다르면 downsample로 맞춤\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity  # skip connection: 입력을 결과에 더함\n",
        "        out = self.relu(out)  # ReLU 활성화\n",
        "\n",
        "        return out\n",
        "\n",
        "# 전체 ResNet-18 모델 정의\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64  # 초기 입력 채널 수 (conv1 출력 채널)\n",
        "\n",
        "        # CIFAR-10은 32x32 이미지라서 kernel=3, stride=1, padding=1 사용 (이미지 크기 유지)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # CIFAR-10 특성상 maxpool 생략 가능 (원하면 추가 가능)\n",
        "        # self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # 각 레이어 별로 ResidualBlock 반복 횟수와 출력 채널 수 지정\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)  # stride=2로 다운샘플링 (절반 크기)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # 공간 크기를 1x1로 줄임\n",
        "        self.fc = nn.Linear(512, num_classes)  # 최종 클래스 분류용 완전연결층\n",
        "\n",
        "    # ResidualBlock 레이어를 여러 개 쌓는 함수\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        # 입력 채널 수와 출력 채널 수가 다르거나 stride가 1이 아니면 downsample 적용\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "        layers = []\n",
        "        # 첫 번째 블록에서는 stride, downsample 적용\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        # 두 번째 이후 블록들은 stride=1, downsample 없음\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)  # nn.Sequential로 묶음\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)  # 초기 conv 연산\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        # CIFAR-10은 maxpool 생략(이유:데이터 크기가 작아서 크기를 줄이면 정보가 많이 소실됨)\n",
        "        # x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)  # ResidualBlock 층 쌓기\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)  # 공간 크기 1x1로 줄이기\n",
        "        x = torch.flatten(x, 1)  # 배치 차원 제외하고 flatten\n",
        "        x = self.fc(x)  # 최종 완전연결층 통과\n",
        "\n",
        "        return x\n",
        "\n",
        "# ResNet-18 생성 함수: ResidualBlock 2개씩 4개 층으로 구성\n",
        "def resnet18(num_classes=10):\n",
        "    return ResNet(ResidualBlock, [2, 2, 2, 2], num_classes)\n",
        "\n",
        "# 모델 생성 및 device 할당\n",
        "model = resnet18(num_classes=10).to(device)\n",
        "\n",
        "# 손실 함수로 크로스엔트로피 설정\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD 옵티마이저 설정: 학습률 0.1, 모멘텀 0.9, 가중치 감쇠 5e-4\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# 학습률 스케줄러: 30, 60, 90 에폭마다 0.1배씩 감소(학습률learning rate를 조정 하는것,이유:학습률이 크면 발산하고 작으면 너무 느리니 적당한 값을 찾는것은 중요 0.1배씩 감소 한다는것은 학습률이 1이면 30번째 에포치 부터는 학습률 0.1 60은 0.01 90은 0.001이다.\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                           milestones=[30, 60, 90], gamma=0.1)\n",
        "\n",
        "# 학습 함수\n",
        "def train(epoch):\n",
        "    model.train()  # 학습 모드 설정 (드롭아웃, 배치정규화 등 활성화)\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    print(f\"Epoch {epoch} 시작\")\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # GPU로 데이터 이동\n",
        "\n",
        "        optimizer.zero_grad()  # 기울기 초기화\n",
        "        outputs = model(inputs)  # 순전파\n",
        "        loss = criterion(outputs, targets)  # 손실 계산\n",
        "        loss.backward()  # 역전파로 기울기 계산\n",
        "        optimizer.step()  # 가중치 갱신\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)  # 예측 결과\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # 100 미니배치마다 손실과 정확도 출력\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"  Batch {batch_idx+1}, Loss: {running_loss / 100:.3f}, Accuracy: {100.*correct/total:.2f}%\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# 테스트 함수\n",
        "def test(epoch):\n",
        "    model.eval()  # 평가 모드 설정 (드롭아웃 등 비활성화)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():  # 기울기 계산 안함\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch} 테스트: Loss: {test_loss/len(testloader):.3f}, Accuracy: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# 총 학습 횟수 (에폭) 설정\n",
        "num_epochs = 100\n",
        "\n",
        "# 학습 및 테스트 반복\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()  # 학습률 스케줄러 업데이트\n"
      ],
      "metadata": {
        "id": "KbwuG-_JzM1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}