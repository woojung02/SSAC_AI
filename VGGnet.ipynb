{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwJqDwEqJ4rouCdnsqpxTP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woojung02/SSAC_AI/blob/main/VGGnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbB84yo0zmSC",
        "outputId": "651e4ae0-ab49-437e-c964-0c477f5978db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][100] Loss: 2.135, Acc: 17.96%\n",
            "[1][200] Loss: 1.883, Acc: 22.20%\n",
            "[1][300] Loss: 1.726, Acc: 25.32%\n",
            "==> Epoch 1 Test Loss: 1.570, Acc: 41.55%\n",
            "[2][100] Loss: 1.547, Acc: 41.34%\n",
            "[2][200] Loss: 1.444, Acc: 43.77%\n",
            "[2][300] Loss: 1.383, Acc: 45.89%\n",
            "==> Epoch 2 Test Loss: 1.290, Acc: 53.18%\n",
            "[3][100] Loss: 1.235, Acc: 56.34%\n",
            "[3][200] Loss: 1.166, Acc: 57.79%\n",
            "[3][300] Loss: 1.073, Acc: 59.47%\n",
            "==> Epoch 3 Test Loss: 1.044, Acc: 64.54%\n",
            "[4][100] Loss: 0.995, Acc: 65.79%\n",
            "[4][200] Loss: 0.935, Acc: 66.91%\n",
            "[4][300] Loss: 0.890, Acc: 67.95%\n",
            "==> Epoch 4 Test Loss: 1.215, Acc: 62.86%\n",
            "[5][100] Loss: 0.854, Acc: 71.64%\n",
            "[5][200] Loss: 0.814, Acc: 72.31%\n",
            "[5][300] Loss: 0.786, Acc: 72.71%\n",
            "==> Epoch 5 Test Loss: 1.258, Acc: 62.83%\n",
            "[6][100] Loss: 0.738, Acc: 76.30%\n",
            "[6][200] Loss: 0.692, Acc: 76.89%\n",
            "[6][300] Loss: 0.705, Acc: 76.92%\n",
            "==> Epoch 6 Test Loss: 0.666, Acc: 77.82%\n",
            "[7][100] Loss: 0.663, Acc: 78.41%\n",
            "[7][200] Loss: 0.666, Acc: 78.49%\n",
            "[7][300] Loss: 0.659, Acc: 78.52%\n",
            "==> Epoch 7 Test Loss: 0.759, Acc: 76.23%\n",
            "[8][100] Loss: 0.608, Acc: 79.98%\n",
            "[8][200] Loss: 0.600, Acc: 80.38%\n",
            "[8][300] Loss: 0.593, Acc: 80.52%\n",
            "==> Epoch 8 Test Loss: 0.698, Acc: 77.69%\n",
            "[9][100] Loss: 0.561, Acc: 82.17%\n",
            "[9][200] Loss: 0.572, Acc: 81.82%\n",
            "[9][300] Loss: 0.568, Acc: 81.63%\n",
            "==> Epoch 9 Test Loss: 0.798, Acc: 75.39%\n",
            "[10][100] Loss: 0.541, Acc: 82.30%\n",
            "[10][200] Loss: 0.544, Acc: 82.27%\n",
            "[10][300] Loss: 0.541, Acc: 82.29%\n",
            "==> Epoch 10 Test Loss: 0.808, Acc: 74.53%\n",
            "[11][100] Loss: 0.505, Acc: 83.48%\n",
            "[11][200] Loss: 0.526, Acc: 83.36%\n",
            "[11][300] Loss: 0.520, Acc: 83.20%\n",
            "==> Epoch 11 Test Loss: 0.662, Acc: 78.60%\n",
            "[12][100] Loss: 0.484, Acc: 84.42%\n",
            "[12][200] Loss: 0.487, Acc: 84.29%\n",
            "[12][300] Loss: 0.491, Acc: 84.19%\n",
            "==> Epoch 12 Test Loss: 0.834, Acc: 74.24%\n",
            "[13][100] Loss: 0.467, Acc: 84.90%\n",
            "[13][200] Loss: 0.476, Acc: 84.82%\n",
            "[13][300] Loss: 0.469, Acc: 84.86%\n",
            "==> Epoch 13 Test Loss: 0.664, Acc: 79.68%\n",
            "[14][100] Loss: 0.443, Acc: 85.49%\n",
            "[14][200] Loss: 0.460, Acc: 85.25%\n",
            "[14][300] Loss: 0.463, Acc: 85.31%\n",
            "==> Epoch 14 Test Loss: 0.556, Acc: 82.12%\n",
            "[15][100] Loss: 0.438, Acc: 86.03%\n",
            "[15][200] Loss: 0.441, Acc: 85.95%\n",
            "[15][300] Loss: 0.452, Acc: 85.74%\n",
            "==> Epoch 15 Test Loss: 0.518, Acc: 83.27%\n",
            "[16][100] Loss: 0.406, Acc: 86.82%\n",
            "[16][200] Loss: 0.442, Acc: 86.27%\n",
            "[16][300] Loss: 0.436, Acc: 86.10%\n",
            "==> Epoch 16 Test Loss: 0.499, Acc: 83.90%\n",
            "[17][100] Loss: 0.408, Acc: 86.82%\n",
            "[17][200] Loss: 0.411, Acc: 86.73%\n",
            "[17][300] Loss: 0.417, Acc: 86.74%\n",
            "==> Epoch 17 Test Loss: 0.528, Acc: 82.88%\n",
            "[18][100] Loss: 0.396, Acc: 87.09%\n",
            "[18][200] Loss: 0.415, Acc: 86.89%\n",
            "[18][300] Loss: 0.424, Acc: 86.76%\n",
            "==> Epoch 18 Test Loss: 0.537, Acc: 83.14%\n",
            "[19][100] Loss: 0.400, Acc: 87.32%\n",
            "[19][200] Loss: 0.389, Acc: 87.30%\n",
            "[19][300] Loss: 0.393, Acc: 87.29%\n",
            "==> Epoch 19 Test Loss: 0.522, Acc: 83.75%\n",
            "[20][100] Loss: 0.374, Acc: 87.88%\n",
            "[20][200] Loss: 0.393, Acc: 87.69%\n",
            "[20][300] Loss: 0.408, Acc: 87.52%\n",
            "==> Epoch 20 Test Loss: 0.562, Acc: 81.90%\n"
          ]
        }
      ],
      "source": [
        "import torch  # PyTorch 핵심 모듈\n",
        "import torch.nn as nn  # 신경망 레이어, 블록 등 제공\n",
        "import torch.optim as optim  # 최적화 함수\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms  # 이미지 전처리, 데이터 증강용\n",
        "#VGGnet-16이용한 코드\n",
        "# 1. device 설정 (GPU가 있으면 GPU 사용, 없으면 CPU 사용)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. CIFAR-10 데이터 전처리\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),   # 입력 이미지를 무작위로 잘라서(패딩 포함) 과적합 방지(32*32를 패딩 하여 40*40으로 바꾸고 위리츷 무작위로 두고 다시 32*32로 자른다,이유:같은 이미지라도 다른 위치로 바꾸면 새로운 샘플처럼 활용 가능,또한 어떤 위치에 어떤 특징이 있다는것을 학습하는것을 막기 위해서 사용)\n",
        "    transforms.RandomHorizontalFlip(),      # 무작위 좌우 반전으로 데이터 다양성 증가(좌우반전해서 같은 이미지라도 샘플 수를 늘릴수 있다.)\n",
        "    transforms.ToTensor(),                  # 이미지를 텐서로 변환 (PyTorch 학습용)\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),  # 평균/표준편차로 정규화 (학습 안정, 속도 증가)\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# 3. CIFAR-10 데이터셋 로드\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# 4. VGG Block 구현 함수\n",
        "def vgg_block(num_convs, in_channels, out_channels):\n",
        "    # num_convs : 연속 합성곱 레이어 수, in_channels : 입력 채널, out_channels : 출력 채널\n",
        "    layers = []\n",
        "    for i in range(num_convs):\n",
        "        # Conv2d: 합성곱 연산, kernel_size=3, padding=1(출력 크기 보존), bias=False(BatchNorm과 궁합)\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(out_channels))  # BatchNorm: 학습 안정성, 수렴 속도 개선\n",
        "        layers.append(nn.ReLU(inplace=True))         # ReLU: 비선형성, 활성화 함수\n",
        "        in_channels = out_channels                   # 다음 conv의 입력 채널을 현재 출력 채널로\n",
        "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))  # MaxPool: 특징 맵 크기 절반으로 줄임(공간 정보 압축)\n",
        "    return nn.Sequential(*layers)                    # 여러 레이어를 묶어서 하나의 블록으로 반환\n",
        "\n",
        "# 5. VGGNet 전체 네트워크 구현\n",
        "class VGGNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGGNet, self).__init__()\n",
        "        # CIFAR-10은 (3,32,32) 이미지 → kernel=3, padding=1로 크기 유지\n",
        "\n",
        "        # VGG-16 레이어 구조: conv개수, 입력/출력 채널 수를 리스트로 관리\n",
        "        conv_arch = [\n",
        "            (2, 3, 64),      # conv 2개, in 3, out 64\n",
        "            (2, 64, 128),    # conv 2개, in 64, out 128\n",
        "            (3, 128, 256),   # conv 3개, in 128, out 256\n",
        "            (3, 256, 512),   # conv 3개, in 256, out 512\n",
        "            (3, 512, 512),   # conv 3개, in 512, out 512\n",
        "        ]\n",
        "\n",
        "        # 합성곱 블록 모듈 쌓기\n",
        "        layers = []\n",
        "        for (num_convs, in_c, out_c) in conv_arch:\n",
        "            layers.append(vgg_block(num_convs, in_c, out_c))\n",
        "        self.features = nn.Sequential(*layers)   # 합성곱 계층 전체를 하나로 묶음\n",
        "\n",
        "        # FC(fully connected) 부분: 특징맵을 일렬로 펼쳐 분류\n",
        "        # CIFAR-10 기준, 마지막 맵 크기는 1x1이 아닌 1x1이 아닐 수 있으니 AdaptiveAvgPool 사용\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # (채널수, 1, 1)로 강제 변환\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096),  # 512(채널) x 1 x 1 = 512 → 4096\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),       # 과적합 방지 드롭아웃\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes), # 최종 클래스 개수만큼 출력(10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # 합성곱(특징추출) 블록 통과\n",
        "        x = self.avgpool(x)   # (B, 512, H, W) → (B, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1)  # (B, 512, 1, 1) → (B, 512)\n",
        "        x = self.classifier(x)   # FC 계층 통과해 분류 결과 출력\n",
        "        return x\n",
        "\n",
        "# 6. 모델 인스턴스 생성, device로 이동\n",
        "model = VGGNet(num_classes=10).to(device)\n",
        "\n",
        "# 7. 손실함수, 옵티마이저, 스케줄러 정의(스케줄러 이용해서 최적의 learning late을 찾기 쉬워진다)\n",
        "criterion = nn.CrossEntropyLoss()   # 다중 클래스 분류용 손실함수\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4) # SGD 옵티마이저\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40, 70], gamma=0.1)  # 학습률 스케줄러\n",
        "\n",
        "# 8. 학습 함수\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()                # 이전 배치의 기울기 리셋\n",
        "        outputs = model(inputs)              # 순전파\n",
        "        loss = criterion(outputs, targets)   # 손실 계산\n",
        "        loss.backward()                     # 역전파(기울기 계산)\n",
        "        optimizer.step()                    # 파라미터 업데이트\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"[{epoch}][{batch_idx+1}] Loss: {running_loss/100:.3f}, Acc: {100.*correct/total:.2f}%\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# 9. 테스트 함수\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    print(f\"==> Epoch {epoch} Test Loss: {test_loss/len(testloader):.3f}, Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# 10. 전체 학습 루프\n",
        "num_epochs = 100\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()  # 학습률 감소 스케줄 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch  # PyTorch 핵심 모듈\n",
        "import torch.nn as nn  # 신경망 레이어, 손실 함수 등\n",
        "import torch.optim as optim  # 최적화 알고리즘\n",
        "import torchvision  # 컴퓨터 비전용 데이터셋, 모델\n",
        "import torchvision.transforms as transforms  # 이미지 전처리, 증강\n",
        "#VGGnet 요약(토치비션에서 불러와서 사용하기)\n",
        "# 1. 장치 설정: GPU가 있으면 'cuda', 없으면 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. CIFAR-10 학습용 전처리 + 증강\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),   # 32×32에 4px 패딩→40×40→랜덤 크롭\n",
        "    transforms.RandomHorizontalFlip(),      # 좌우 반전 증강\n",
        "    transforms.ToTensor(),                  # [0,255]→[0,1] 텐서 변환\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),  # 채널별 평균 정규화\n",
        "                         (0.2023,0.1994,0.2010))  # 채널별 표준편차 정규화\n",
        "])\n",
        "\n",
        "# 3. CIFAR-10 테스트용 전처리\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.2023,0.1994,0.2010))\n",
        "])\n",
        "\n",
        "# 4. 데이터셋 로드 및 DataLoader 생성\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# 5. torchvision의 VGG-16 불러오기\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "model = vgg16(pretrained=False)             # 사전학습된 가중치 없이 초기화\n",
        "model.classifier[-1] = nn.Linear(4096, 10)   # 마지막 FC를 CIFAR-10(10클래스)에 맞게 교체\n",
        "model = model.to(device)                    # 모델을 GPU/CPU로 이동\n",
        "\n",
        "# 6. 손실함수와 옵티마이저, 스케줄러 정의\n",
        "criterion = nn.CrossEntropyLoss()            # 분류용 손실\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05,  # 학습률 0.05\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                           milestones=[40, 70], gamma=0.1)\n",
        "\n",
        "# 7.train함수 정의\n",
        "def train(epoch):\n",
        "    model.train()  # 학습 모드 활성화 (드롭아웃·배치정규화 작동)\n",
        "    running_loss = 0.0\n",
        "    correct = total = 0\n",
        "    print(f\"Epoch {epoch} 시작\")\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # GPU로 이동\n",
        "        optimizer.zero_grad()             # 이전 기울기 초기화\n",
        "        outputs = model(inputs)           # 순전파\n",
        "        loss = criterion(outputs, targets)# 손실 계산\n",
        "        loss.backward()                   # 역전파\n",
        "        optimizer.step()                  # 파라미터 업데이트\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)     # 가장 높은 확률 클래스 선택\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # 100배치마다 로그\n",
        "        if batch_idx % 100 == 99:\n",
        "            print(f\"  Batch {batch_idx+1}, \"\n",
        "                  f\"Loss: {running_loss/100:.3f}, \"\n",
        "                  f\"Acc: {100.*correct/total:.2f}%\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "# 8. 테스트\n",
        "def test(epoch):\n",
        "    model.eval()  # 평가 모드 활성화 (드롭아웃·배치정규화 비활성화)\n",
        "    test_loss = correct = total = 0\n",
        "    with torch.no_grad():  # 기울기 계산 비활성화\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(f\"==> Epoch {epoch} Test Loss: {test_loss/len(testloader):.3f}, \"\n",
        "          f\"Acc: {100.*correct/total:.2f}%\")\n",
        "\n",
        "# 9. 전체 학습 루프\n",
        "num_epochs = 100\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train(epoch)      # 학습\n",
        "    test(epoch)       # 평가\n",
        "    scheduler.step()  # 학습률 스케줄러 갱신\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1MI46DQ9b8_",
        "outputId": "8d535506-fa5e-44f8-f889-72c5dc31b7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 시작\n",
            "  Batch 100, Loss: 2.277, Acc: 13.09%\n",
            "  Batch 200, Loss: 2.228, Acc: 13.93%\n",
            "  Batch 300, Loss: 2.117, Acc: 15.33%\n",
            "==> Epoch 1 Test Loss: 1.993, Acc: 18.06%\n",
            "Epoch 2 시작\n",
            "  Batch 100, Loss: 1.954, Acc: 20.88%\n",
            "  Batch 200, Loss: 1.864, Acc: 22.84%\n",
            "  Batch 300, Loss: 1.817, Acc: 24.49%\n",
            "==> Epoch 2 Test Loss: 1.822, Acc: 31.13%\n",
            "Epoch 3 시작\n",
            "  Batch 100, Loss: 1.753, Acc: 31.80%\n",
            "  Batch 200, Loss: 1.720, Acc: 32.50%\n",
            "  Batch 300, Loss: 1.628, Acc: 33.92%\n",
            "==> Epoch 3 Test Loss: 1.554, Acc: 38.18%\n",
            "Epoch 4 시작\n",
            "  Batch 100, Loss: 1.531, Acc: 41.72%\n",
            "  Batch 200, Loss: 1.519, Acc: 42.62%\n",
            "  Batch 300, Loss: 1.518, Acc: 42.97%\n",
            "==> Epoch 4 Test Loss: 1.252, Acc: 54.26%\n",
            "Epoch 5 시작\n",
            "  Batch 100, Loss: 1.386, Acc: 50.63%\n",
            "  Batch 200, Loss: 1.339, Acc: 51.75%\n",
            "  Batch 300, Loss: 1.273, Acc: 53.24%\n",
            "==> Epoch 5 Test Loss: 1.201, Acc: 58.16%\n",
            "Epoch 6 시작\n",
            "  Batch 100, Loss: 1.199, Acc: 58.71%\n",
            "  Batch 200, Loss: 1.119, Acc: 60.40%\n",
            "  Batch 300, Loss: 1.128, Acc: 60.77%\n",
            "==> Epoch 6 Test Loss: 1.010, Acc: 65.55%\n",
            "Epoch 7 시작\n",
            "  Batch 100, Loss: 1.055, Acc: 63.99%\n",
            "  Batch 200, Loss: 1.030, Acc: 64.69%\n",
            "  Batch 300, Loss: 0.986, Acc: 65.42%\n",
            "==> Epoch 7 Test Loss: 0.968, Acc: 67.88%\n",
            "Epoch 8 시작\n",
            "  Batch 100, Loss: 0.999, Acc: 66.73%\n",
            "  Batch 200, Loss: 0.943, Acc: 67.79%\n",
            "  Batch 300, Loss: 0.925, Acc: 68.50%\n",
            "==> Epoch 8 Test Loss: 0.872, Acc: 71.45%\n",
            "Epoch 9 시작\n",
            "  Batch 100, Loss: 0.854, Acc: 72.15%\n",
            "  Batch 200, Loss: 0.882, Acc: 71.60%\n",
            "  Batch 300, Loss: 0.852, Acc: 71.80%\n",
            "==> Epoch 9 Test Loss: 0.830, Acc: 72.87%\n",
            "Epoch 10 시작\n",
            "  Batch 100, Loss: 0.838, Acc: 72.90%\n",
            "  Batch 200, Loss: 0.812, Acc: 73.37%\n",
            "  Batch 300, Loss: 0.804, Acc: 73.76%\n",
            "==> Epoch 10 Test Loss: 0.759, Acc: 76.63%\n",
            "Epoch 11 시작\n",
            "  Batch 100, Loss: 0.768, Acc: 75.59%\n",
            "  Batch 200, Loss: 0.791, Acc: 75.05%\n",
            "  Batch 300, Loss: 0.739, Acc: 75.38%\n",
            "==> Epoch 11 Test Loss: 0.788, Acc: 74.12%\n",
            "Epoch 12 시작\n",
            "  Batch 100, Loss: 0.732, Acc: 76.44%\n",
            "  Batch 200, Loss: 0.733, Acc: 76.53%\n",
            "  Batch 300, Loss: 0.736, Acc: 76.59%\n",
            "==> Epoch 12 Test Loss: 0.684, Acc: 78.20%\n",
            "Epoch 13 시작\n",
            "  Batch 100, Loss: 0.696, Acc: 77.87%\n",
            "  Batch 200, Loss: 0.696, Acc: 77.78%\n",
            "  Batch 300, Loss: 0.680, Acc: 78.05%\n",
            "==> Epoch 13 Test Loss: 0.644, Acc: 79.48%\n",
            "Epoch 14 시작\n",
            "  Batch 100, Loss: 0.659, Acc: 79.21%\n",
            "  Batch 200, Loss: 0.682, Acc: 78.91%\n",
            "  Batch 300, Loss: 0.679, Acc: 78.77%\n",
            "==> Epoch 14 Test Loss: 0.650, Acc: 79.01%\n",
            "Epoch 15 시작\n",
            "  Batch 100, Loss: 0.628, Acc: 80.23%\n",
            "  Batch 200, Loss: 0.639, Acc: 80.04%\n",
            "  Batch 300, Loss: 0.650, Acc: 79.92%\n",
            "==> Epoch 15 Test Loss: 0.622, Acc: 80.63%\n",
            "Epoch 16 시작\n",
            "  Batch 100, Loss: 0.645, Acc: 79.27%\n",
            "  Batch 200, Loss: 0.629, Acc: 79.71%\n",
            "  Batch 300, Loss: 0.631, Acc: 79.94%\n",
            "==> Epoch 16 Test Loss: 0.632, Acc: 80.13%\n",
            "Epoch 17 시작\n",
            "  Batch 100, Loss: 0.609, Acc: 81.08%\n",
            "  Batch 200, Loss: 0.623, Acc: 80.61%\n",
            "  Batch 300, Loss: 0.606, Acc: 80.66%\n",
            "==> Epoch 17 Test Loss: 0.568, Acc: 81.93%\n",
            "Epoch 18 시작\n",
            "  Batch 100, Loss: 0.585, Acc: 81.48%\n",
            "  Batch 200, Loss: 0.587, Acc: 81.73%\n"
          ]
        }
      ]
    }
  ]
}