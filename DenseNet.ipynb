{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3KZNfb1L+lsIKLVA0ui/4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/woojung02/SSAC_AI/blob/main/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRRrQj1-qLH0",
        "outputId": "62ed0bfe-4873-4a0a-f941-efaa7a330cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 45.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 1: Loss=1.4637, Acc=45.77%\n",
            "[Test ] Epoch 1: Loss=1.3252, Acc=55.16%\n",
            "[Train] Epoch 2: Loss=0.9811, Acc=65.14%\n",
            "[Test ] Epoch 2: Loss=1.0302, Acc=64.70%\n",
            "[Train] Epoch 3: Loss=0.8015, Acc=71.81%\n",
            "[Test ] Epoch 3: Loss=0.7716, Acc=73.64%\n",
            "[Train] Epoch 4: Loss=0.6889, Acc=76.10%\n",
            "[Test ] Epoch 4: Loss=0.7338, Acc=75.48%\n",
            "[Train] Epoch 5: Loss=0.6140, Acc=78.73%\n",
            "[Test ] Epoch 5: Loss=0.7166, Acc=75.89%\n",
            "[Train] Epoch 6: Loss=0.5710, Acc=80.25%\n",
            "[Test ] Epoch 6: Loss=0.7046, Acc=76.33%\n",
            "[Train] Epoch 7: Loss=0.5229, Acc=81.93%\n",
            "[Test ] Epoch 7: Loss=0.6138, Acc=79.66%\n",
            "[Train] Epoch 8: Loss=0.4962, Acc=83.03%\n",
            "[Test ] Epoch 8: Loss=0.6807, Acc=77.94%\n",
            "[Train] Epoch 9: Loss=0.4726, Acc=83.59%\n",
            "[Test ] Epoch 9: Loss=0.6252, Acc=79.21%\n",
            "[Train] Epoch 10: Loss=0.4483, Acc=84.46%\n",
            "[Test ] Epoch 10: Loss=0.6086, Acc=79.88%\n",
            "[Train] Epoch 11: Loss=0.4317, Acc=85.00%\n",
            "[Test ] Epoch 11: Loss=0.5693, Acc=81.71%\n",
            "[Train] Epoch 12: Loss=0.4211, Acc=85.34%\n",
            "[Test ] Epoch 12: Loss=0.5632, Acc=81.53%\n",
            "[Train] Epoch 13: Loss=0.4013, Acc=86.20%\n",
            "[Test ] Epoch 13: Loss=0.4910, Acc=83.58%\n",
            "[Train] Epoch 14: Loss=0.3898, Acc=86.64%\n",
            "[Test ] Epoch 14: Loss=0.5558, Acc=81.37%\n",
            "[Train] Epoch 15: Loss=0.3837, Acc=86.82%\n",
            "[Test ] Epoch 15: Loss=0.4685, Acc=84.43%\n",
            "[Train] Epoch 16: Loss=0.3716, Acc=87.04%\n",
            "[Test ] Epoch 16: Loss=0.5080, Acc=83.15%\n",
            "[Train] Epoch 17: Loss=0.3677, Acc=87.25%\n",
            "[Test ] Epoch 17: Loss=0.6917, Acc=78.45%\n",
            "[Train] Epoch 18: Loss=0.3589, Acc=87.66%\n",
            "[Test ] Epoch 18: Loss=0.6013, Acc=81.21%\n",
            "[Train] Epoch 19: Loss=0.3499, Acc=87.84%\n",
            "[Test ] Epoch 19: Loss=0.5005, Acc=83.40%\n",
            "[Train] Epoch 20: Loss=0.3449, Acc=88.05%\n",
            "[Test ] Epoch 20: Loss=0.5244, Acc=82.98%\n"
          ]
        }
      ],
      "source": [
        "import torch                                   # PyTorch 기본 모듈\n",
        "import torch.nn as nn                          # 신경망 레이어(nn.Module, Conv2d, BatchNorm2d, Linear 등)\n",
        "import torch.optim as optim                     # 최적화 알고리즘 (SGD, Adam 등)\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms    # 데이터 전처리 & 증강\n",
        "from torch.utils.data import DataLoader       # 데이터 로더\n",
        "\n",
        "# 1) 장치 설정 (GPU 사용 가능하면 GPU, 아니면 CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2) CIFAR-10 전처리 및 증강 정의\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),      # 작은 패딩을 붙이고 랜덤 크롭 → 과적합 방지\n",
        "    transforms.RandomHorizontalFlip(),         # 좌우 반전 → 데이터 다양성 증가\n",
        "    transforms.ToTensor(),                     # [0,255]→[0,1] Tensor\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),# 채널별 평균으로 정규화 → 학습 안정화\n",
        "                         (0.2023,0.1994,0.2010)),# 채널별 표준편차로 스케일링\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
        "])\n",
        "\n",
        "# 3) 데이터셋 & 로더 준비\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "testset  = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                        download=True, transform=transform_test)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True,  num_workers=2)\n",
        "testloader  = DataLoader(testset,  batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# 4) Dense Layer (Bottleneck) 정의\n",
        "class _DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, bn_size, drop_rate):\n",
        "        super().__init__()\n",
        "        # 1×1 Bottleneck conv (채널 압축)\n",
        "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, bn_size*growth_rate,\n",
        "                               kernel_size=1, stride=1, bias=False)\n",
        "        # 3×3 conv (실제 특징 추출)\n",
        "        self.bn2   = nn.BatchNorm2d(bn_size*growth_rate)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(bn_size*growth_rate, growth_rate,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, in_channels, H, W]\n",
        "        out = self.conv1(self.relu1(self.bn1(x)))\n",
        "        out = self.conv2(self.relu2(self.bn2(out)))\n",
        "        if self.drop_rate > 0:\n",
        "            out = nn.functional.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        # Dense connection: 입력에 새로 만든 특징 맵을 채널 차원으로 이어붙임\n",
        "        return torch.cat([x, out], 1)\n",
        "\n",
        "# 5) Dense Block 정의 (여러 개의 DenseLayer를 쌓음)\n",
        "class _DenseBlock(nn.Module):\n",
        "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            layers.append(_DenseLayer(\n",
        "                in_channels + i*growth_rate,  # 매 레이어마다 채널이 growth_rate만큼 늘어남\n",
        "                growth_rate, bn_size, drop_rate\n",
        "            ))\n",
        "        self.denseblock = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.denseblock(x)\n",
        "\n",
        "# 6) Transition Layer 정의 (채널/공간 크기 축소)\n",
        "class _Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.bn    = nn.BatchNorm2d(in_channels)\n",
        "        self.relu  = nn.ReLU(inplace=True)\n",
        "        self.conv  = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=1, stride=1, bias=False)\n",
        "        self.pool  = nn.AvgPool2d(kernel_size=2, stride=2)  # 절반 크기로 다운샘플링\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool(self.conv(self.relu(self.bn(x))))\n",
        "\n",
        "# 7) 전체 DenseNet 정의\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=12, block_layers=(6,6,6), # CIFAR용 작은 DenseNet-BC\n",
        "                 num_init_features=2*12, bn_size=4, drop_rate=0, num_classes=10):\n",
        "        super().__init__()\n",
        "        # (1) 초기 컨브: 3채널→num_init_features, 크기 유지(padding=1)\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, num_init_features, kernel_size=3,\n",
        "                      stride=1, padding=1, bias=False),\n",
        "        )\n",
        "        num_features = num_init_features\n",
        "\n",
        "        # (2) DenseBlock + Transition 쌓기\n",
        "        for i, num_layers in enumerate(block_layers):\n",
        "            # DenseBlock\n",
        "            block = _DenseBlock(num_layers, num_features, growth_rate, bn_size, drop_rate)\n",
        "            self.features.add_module(f\"denseblock{i+1}\", block)\n",
        "            num_features += num_layers * growth_rate\n",
        "\n",
        "            # 마지막 블록 뒤에는 Transition 추가하고 싶지 않음\n",
        "            if i != len(block_layers)-1:\n",
        "                trans = _Transition(num_features, num_features // 2)\n",
        "                self.features.add_module(f\"transition{i+1}\", trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # (3) 최종 BatchNorm+ReLU\n",
        "        self.features.add_module(\"norm_final\", nn.BatchNorm2d(num_features))\n",
        "        self.features.add_module(\"relu_final\", nn.ReLU(inplace=True))\n",
        "\n",
        "        # (4) 분류기: 전역 평균 풀링 + FC\n",
        "        self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # 가중치 초기화\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)  # He 초기화\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)     # scale=1\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 특징 추출\n",
        "        features = self.features(x)          # [B, C, H, W]\n",
        "        # 전역 평균 풀링 (H×W → 1×1)\n",
        "        out = nn.functional.adaptive_avg_pool2d(features, (1,1))\n",
        "        out = torch.flatten(out, 1)          # [B, C]\n",
        "        # 분류기\n",
        "        return self.classifier(out)\n",
        "\n",
        "# 8) 모델 생성 & 옵티마이저/손실함수/스케줄러\n",
        "model = DenseNet().to(device)               # CIFAR-10용 기본 설정\n",
        "criterion = nn.CrossEntropyLoss()           # 분류용 손실\n",
        "optimizer = optim.SGD(model.parameters(),\n",
        "                      lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=[50,100,150], gamma=0.1)\n",
        "\n",
        "# 9) 학습 루프 & 평가 함수\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss    += loss.item()*images.size(0)\n",
        "        total_correct += (outputs.argmax(1)==labels).sum().item()\n",
        "\n",
        "    print(f\"[Train] Epoch {epoch}: \"\n",
        "          f\"Loss={total_loss/len(trainset):.4f}, \"\n",
        "          f\"Acc={100*total_correct/len(trainset):.2f}%\")\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss    += loss.item()*images.size(0)\n",
        "            total_correct += (outputs.argmax(1)==labels).sum().item()\n",
        "\n",
        "    print(f\"[Test ] Epoch {epoch}: \"\n",
        "          f\"Loss={total_loss/len(testset):.4f}, \"\n",
        "          f\"Acc={100*total_correct/len(testset):.2f}%\")\n",
        "\n",
        "# 10) 전체 학습 실행\n",
        "num_epochs = 100\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                                      # PyTorch 기본 모듈\n",
        "import torch.nn as nn                             # 신경망 레이어 & 손실함수\n",
        "import torch.optim as optim                        # 최적화 알고리즘\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms      # 데이터셋 & 전처리\n",
        "from torch.utils.data import DataLoader            # 배치 단위 로더\n",
        "from torchvision.models import densenet121         # TorchVision 제공 DenseNet-121\n",
        "\n",
        "# 1) 장치 설정: GPU 가능 시 사용\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2) CIFAR-10 전처리 정의 (학습 시 증강 포함)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),    # 패딩 후 랜덤 크롭: 과적합 방지\n",
        "    transforms.RandomHorizontalFlip(),       # 좌우 뒤집기: 데이터 다양성\n",
        "    transforms.ToTensor(),                   # [0,255]→[0,1] 텐서 변환\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),  # 채널별 평균으로 정규화\n",
        "                         (0.2023,0.1994,0.2010))  # 채널별 표준편차로 스케일링\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914,0.4822,0.4465),\n",
        "                         (0.2023,0.1994,0.2010)),\n",
        "])\n",
        "\n",
        "# 3) 데이터셋 & 데이터로더 생성\n",
        "trainset    = datasets.CIFAR10(root='./data', train=True,  download=True, transform=transform_train)\n",
        "testset     = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True,  num_workers=2)\n",
        "testloader  = DataLoader(testset,  batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# 4) 사전 정의된 DenseNet-121 모델 불러와서 CIFAR-10용으로 수정\n",
        "model = densenet121(pretrained=False, num_classes=10).to(device)\n",
        "\n",
        "# 5) 손실함수 & 옵티마이저 & 스케줄러 설정\n",
        "criterion = nn.CrossEntropyLoss()                  # 다중 클래스 분류 손실\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100,150], gamma=0.1)\n",
        "# milestones마다 lr 0.1배 감소\n",
        "\n",
        "# 6) 학습 함수\n",
        "def train(epoch):\n",
        "    model.train()                                 # 드롭아웃/배치정규화 활성화\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for imgs, labels in trainloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()                    # 기울기 초기화\n",
        "        outputs = model(imgs)                    # 순전파\n",
        "        loss = criterion(outputs, labels)        # 손실 계산\n",
        "        loss.backward()                          # 역전파\n",
        "        optimizer.step()                         # 파라미터 업데이트\n",
        "\n",
        "        running_loss += loss.item()*labels.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += preds.eq(labels).sum().item()\n",
        "        total   += labels.size(0)\n",
        "    print(f\"[Train] Epoch {epoch}  Loss: {running_loss/total:.4f}  Acc: {100*correct/total:.2f}%\")\n",
        "\n",
        "# 7) 평가 함수\n",
        "def test(epoch):\n",
        "    model.eval()                                 # 평가 모드: 드롭아웃·배치정규화 비활성화\n",
        "    loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in testloader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss += criterion(outputs, labels).item()*labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += preds.eq(labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    print(f\"[Test ] Epoch {epoch}  Loss: {loss/total:.4f}  Acc: {100*correct/total:.2f}%\")\n",
        "\n",
        "# 8) 전체 학습 루프\n",
        "num_epochs = 20\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    train(epoch)               # 학습 단계\n",
        "    test(epoch)                # 테스트 단계\n",
        "    scheduler.step()           # 학습률 스케줄러 업데이트\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJeqorBc0YLA",
        "outputId": "ccad3acd-10e6-418a-974d-b872e9a5b915"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Train] Epoch 1  Loss: 2.1026  Acc: 31.15%\n",
            "[Test ] Epoch 1  Loss: 1.7941  Acc: 35.32%\n",
            "[Train] Epoch 2  Loss: 1.5169  Acc: 44.39%\n",
            "[Test ] Epoch 2  Loss: 1.4341  Acc: 48.29%\n",
            "[Train] Epoch 3  Loss: 1.3351  Acc: 51.58%\n",
            "[Test ] Epoch 3  Loss: 1.2946  Acc: 53.58%\n",
            "[Train] Epoch 4  Loss: 1.2052  Acc: 56.88%\n",
            "[Test ] Epoch 4  Loss: 1.1116  Acc: 60.63%\n",
            "[Train] Epoch 5  Loss: 1.0668  Acc: 61.75%\n",
            "[Test ] Epoch 5  Loss: 0.9663  Acc: 65.59%\n",
            "[Train] Epoch 6  Loss: 0.9932  Acc: 64.84%\n",
            "[Test ] Epoch 6  Loss: 0.9881  Acc: 65.19%\n",
            "[Train] Epoch 7  Loss: 0.9601  Acc: 65.71%\n",
            "[Test ] Epoch 7  Loss: 0.9266  Acc: 67.86%\n",
            "[Train] Epoch 8  Loss: 0.8939  Acc: 68.34%\n",
            "[Test ] Epoch 8  Loss: 0.8519  Acc: 69.83%\n",
            "[Train] Epoch 9  Loss: 0.8303  Acc: 70.66%\n",
            "[Test ] Epoch 9  Loss: 0.8273  Acc: 71.24%\n",
            "[Train] Epoch 10  Loss: 1.0293  Acc: 63.53%\n",
            "[Test ] Epoch 10  Loss: 1.2369  Acc: 55.75%\n",
            "[Train] Epoch 11  Loss: 1.0712  Acc: 61.99%\n",
            "[Test ] Epoch 11  Loss: 0.9456  Acc: 66.76%\n",
            "[Train] Epoch 12  Loss: 0.8792  Acc: 69.04%\n",
            "[Test ] Epoch 12  Loss: 0.8769  Acc: 70.32%\n",
            "[Train] Epoch 13  Loss: 0.8095  Acc: 71.46%\n",
            "[Test ] Epoch 13  Loss: 0.7816  Acc: 72.58%\n",
            "[Train] Epoch 14  Loss: 0.7491  Acc: 73.85%\n",
            "[Test ] Epoch 14  Loss: 0.7528  Acc: 74.21%\n",
            "[Train] Epoch 15  Loss: 0.7114  Acc: 74.98%\n",
            "[Test ] Epoch 15  Loss: 0.7178  Acc: 74.96%\n",
            "[Train] Epoch 16  Loss: 0.6754  Acc: 76.22%\n",
            "[Test ] Epoch 16  Loss: 0.8602  Acc: 73.44%\n",
            "[Train] Epoch 17  Loss: 0.6476  Acc: 77.09%\n",
            "[Test ] Epoch 17  Loss: 0.7415  Acc: 75.19%\n",
            "[Train] Epoch 18  Loss: 0.6269  Acc: 77.90%\n",
            "[Test ] Epoch 18  Loss: 0.6902  Acc: 76.58%\n",
            "[Train] Epoch 19  Loss: 0.6001  Acc: 79.04%\n",
            "[Test ] Epoch 19  Loss: 0.6708  Acc: 77.49%\n",
            "[Train] Epoch 20  Loss: 0.5747  Acc: 79.69%\n",
            "[Test ] Epoch 20  Loss: 0.6441  Acc: 78.23%\n"
          ]
        }
      ]
    }
  ]
}